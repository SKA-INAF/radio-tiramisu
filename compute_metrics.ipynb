{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.rg_data import AstroDataLoaders\n",
    "from pathlib import Path\n",
    "from utils.logging import Logger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import tiramisu\n",
    "from scipy import ndimage\n",
    "import utils.training as train_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = 'latest.pth'\n",
    "DATA_PATH = Path('data_reduced')\n",
    "WEIGHTS_PATH = Path('weights')\n",
    "results_dir = 'results'\n",
    "log_file = 'log.txt'\n",
    "batch_size = 20\n",
    "n_classes = 4\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = AstroDataLoaders(DATA_PATH, batch_size)\n",
    "test_loader = data_loader.get_test_loader()\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(0)\n",
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tiramisu.FCDenseNet67(n_classes=n_classes).to(device)\n",
    "criterion = nn.NLLLoss(weight=data_loader.class_weight.cuda()).cuda()\n",
    "train_utils.load_weights(model, str(WEIGHTS_PATH)+'/' + resume)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "logger = Logger(log_file, test_loader.dataset.classes, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = ['union', 'tp', 'fp', 'fn', 'obj_tp', 'obj_fp', 'obj_fn']\n",
    "metric_names = ['accuracy', 'iou', 'precision', 'recall', 'dice', 'obj_precision', 'obj_recall']\n",
    "classes = ['Void', 'Sidelobe', 'Source', 'Galaxy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(output_batch):\n",
    "    bs,c,h,w = output_batch.size()\n",
    "    tensor = output_batch.data\n",
    "    values, indices = tensor.cpu().max(1)\n",
    "    indices = indices.view(bs,h,w)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_union(preds, targets, class_id):\n",
    "    total_union = {}\n",
    "    current_class = torch.where(preds == class_id, 1.,0.) # isolates the class of interest\n",
    "    gt = torch.where(targets == class_id, 1., 0.)\n",
    "    union = torch.where(torch.logical_or(current_class, gt), 1., 0.)\n",
    "\n",
    "    total_union = union.sum().item()\n",
    "    \n",
    "    return total_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(preds, targets, class_id):\n",
    "\n",
    "    assert preds.size() == targets.size()\n",
    "    current_class = preds == class_id # isolates the class of interest\n",
    "    gt = targets == class_id\n",
    "\n",
    "    tp = gt.mul(current_class).eq(1).sum().item()\n",
    "    fp = gt.eq(0).long().mul(current_class).eq(1).sum().item()\n",
    "    fn = current_class.eq(0).long().mul(gt).eq(1).sum().item()\n",
    "    tn = current_class.eq(0).long().mul(gt).eq(0).sum().item()\n",
    "\n",
    "    return tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Object Detection comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_object_confusion_matrix(preds, targets, class_id, threshold=0.5):\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for pred, target in zip(preds, targets):\n",
    "\n",
    "        gt = torch.where(target == class_id, 1., 0.)\n",
    "        current_class = torch.where(pred == class_id, 1., 0.) # isolates the class of interest\n",
    "        pred_objects, nr_pred_objects = ndimage.label(current_class)\n",
    "        target_objects, nr_target_objects = ndimage.label(gt)\n",
    "\n",
    "        for pred_idx in range(nr_pred_objects):\n",
    "            current_obj_pred = torch.where(torch.from_numpy(pred_objects == pred_idx), 1., 0.)\n",
    "\n",
    "            obj_iou = get_obj_iou(nr_target_objects, target_objects, current_obj_pred)\n",
    "            if nr_target_objects != 0:\n",
    "                if obj_iou >= threshold:\n",
    "                    tp += 1\n",
    "                else: \n",
    "                    fp += 1\n",
    "\n",
    "        if nr_target_objects > nr_pred_objects:\n",
    "            fn += (nr_target_objects - nr_pred_objects)\n",
    "    \n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_iou(nr_target_objects, target_objects, current_obj_pred):\n",
    "    obj_ious = []\n",
    "    for target_idx in range(nr_target_objects):\n",
    "        current_obj_target = torch.from_numpy(target_objects == target_idx)\n",
    "        intersection = torch.where(torch.logical_and(current_obj_pred, current_obj_target), 1., 0.)\n",
    "        union = torch.where(torch.logical_or(current_obj_pred, current_obj_target), 1., 0.)\n",
    "\n",
    "        obj_ious.append(intersection.sum() / union.sum())\n",
    "    if len(obj_ious) > 0:\n",
    "        return np.nanmax(obj_ious).item()\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate metrics for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def division(x,y):\n",
    "    return x / y if y else 0\n",
    "    \n",
    "def compute_batch_metrics(union, tp, fp, fn, tn):\n",
    "\n",
    "    # TODO IoU and Dice are the same metric, remove?\n",
    "\n",
    "    accuracy       =   division(tp + tn, tp + fp + tn + fn)\n",
    "    iou            =   division(tp, union)\n",
    "    precision      =   division(tp, tp + fp)\n",
    "    recall         =   division(tp, tp + fn)\n",
    "    dice           =   division(tp, tp + fp + fn)\n",
    "\n",
    "    return accuracy, iou, precision, recall, dice\n",
    "\n",
    "def compute_batch_obj_metrics(obj_tp, obj_fp, obj_fn):\n",
    "\n",
    "    obj_precision  =   division(obj_tp, obj_tp + obj_fp)\n",
    "    obj_recall     =   division(obj_tp, obj_tp + obj_fn)\n",
    "\n",
    "    return obj_precision, obj_recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "test_metrics = {class_name: {metric_name: 0. for metric_name in metric_names} for class_name in classes}\n",
    "batch_metrics = {class_name: {metric_name: [] for metric_name in metric_names} for class_name in classes}\n",
    "\n",
    "since = time.time()\n",
    "\n",
    "\n",
    "for data, target in tqdm(test_loader, desc=\"Testing\"):\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device)\n",
    "        targets = target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, targets).item()\n",
    "        preds = get_predictions(output)\n",
    "\n",
    "        # Skipping Background class in metric computation (i + 1)\n",
    "        for i, class_name in enumerate(classes[1:]): \n",
    "            union = compute_union(preds, targets.data.cpu(), i + 1) \n",
    "            if union == 0:\n",
    "                # There is no object with that class, skipping...\n",
    "                continue\n",
    "\n",
    "            tp, fp, fn, tn = compute_confusion_matrix(preds, targets.data.cpu(), i + 1)\n",
    "            obj_tp, obj_fp, obj_fn = compute_object_confusion_matrix(preds, targets.data.cpu(), i + 1)\n",
    "\n",
    "            accuracy, iou, precision, recall, dice = compute_batch_metrics(union, tp, fp, fn, tn)\n",
    "            obj_precision, obj_recall = compute_batch_obj_metrics(obj_tp, obj_fp, obj_fn)\n",
    "\n",
    "            batch_metrics[class_name]['accuracy'].append(accuracy)\n",
    "            batch_metrics[class_name]['iou'].append(iou)\n",
    "            batch_metrics[class_name]['precision'].append(precision)\n",
    "            batch_metrics[class_name]['recall'].append(recall)\n",
    "            batch_metrics[class_name]['dice'].append(dice)\n",
    "            batch_metrics[class_name]['obj_precision'].append(obj_precision)\n",
    "            batch_metrics[class_name]['obj_recall'].append(obj_recall)\n",
    "        \n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "\n",
    "for class_name in classes[1:]:\n",
    "    test_metrics[class_name] = {metric_name: np.mean(batch_metrics[class_name][metric_name]) for metric_name in metric_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.log_metrics('Test', 1, test_loss, test_metrics, time_elapsed)\n",
    "logger.wandb_plot_metrics(test_metrics, 'test')\n",
    "train_utils.view_sample_predictions(model, test_loader, 1, 100, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
